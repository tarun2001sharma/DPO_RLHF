{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450ea553-a2f2-4768-8849-b6168ac1a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.18s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> My favourite condiment is soy sauce. I like the sweet Chinese style ones the best but I also like the salty Japanese ones too. There is a special one I like and it is the Thai style fish sauce.\\n\\nSoy sauce is a great addition to salad dressing… you can use it to make a simple one by mixing some rice vinegar, sesame oil, chili oil, chopped ginger and a tablespoon or two of light soy sauce .\\n\\nAlso a good and'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/scratch/pvg2018/Mistral7b\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/scratch/pvg2018/Mistral7b\")\n",
    "\n",
    "prompt = \"My favourite condiment is\"          \n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\")#.to(\"cuda\")\n",
    "#model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ec6a6b-9ce8-4488-a84f-da3a2d4fdb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBREDDIT: r/relationships\n",
      "TITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\n",
      "POST: Hello hello everybody. I hope this isn't too trivial of a question to ask on here, but I've been feeling a bit out of my depth when it comes to this situation (I've had only one relationship before, and for many reasons, it was out of the ordinary).\n",
      "\n",
      "Okay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We're both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\n",
      "\n",
      "Okay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \n",
      "\n",
      "We ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\n",
      "\n",
      "Yesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I'm wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I've been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n",
      "TL;DR: I really like this guy, but after having sex with him after only knowing him for a very brief period of time, I am worried that I may have ruined my chances of a relationship with him.\n",
      "SUBREDDIT: r/Parenting\n",
      "TITLE: My 11 year old sons friend died suddenly, his funeral is today and my son suddenly doesn't want to attend.\n",
      "POST: **repost from relationships**\n",
      "\n",
      "A couple of weeks ago my sons friend died in a freak accident, it was completely shocking and horrific. He isn't aware of the details, but we broke the news to him as soon as we found out and have spoke about it many times with him.\n",
      "\n",
      "He has cried about it, asked questions and spoken about it with his older siblings (who have also recently lost a friend) and seemed to be okay with it (considering the circumstances).\n",
      "\n",
      "Leading up to the funeral, we have talked about it and explained what he is to expect, etc. This is his first funeral, so we have made sure that he is aware of everything.\n",
      "\n",
      "But today is the day, and he has broken down in the morning and says he doesn't want to go. I have no idea what to do. Do I push him to go? I am worried that he will regret it later, as he is a kid who doesn't like to do new things. But at the same time, I don't want to pressure him to do something he doesn't want to do.\n",
      "Advice, please?\n",
      "TL;DR: Sons good friend died and his funeral is today. Son was always wanting to go until today. Do I push him to go, or let him skip it?\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: The girl [26 F] I [22 M] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend [~30? M].\n",
      "POST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue.\n",
      "\n",
      "I saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time.\n",
      "\n",
      "I don't mind that she hangs out with friends, and I know it's pretty early in the relationship, but am I wrong to be a little annoyed that she didn't respond until 24 hours after my first text?\n",
      "TL;DR:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    text = f\"SUBREDDIT: r/{example['subreddit']}\\nTITLE: {example['title']}\\nPOST: {example['post']}\\nTL;DR: {example['summary']}\"\n",
    "    return text\n",
    "\n",
    "def formatting_prompts_func_without_TLDR(example):\n",
    "    text = f\"SUBREDDIT: r/{example['subreddit']}\\nTITLE: {example['title']}\\nPOST: {example['post']}\\nTL;DR:\"\n",
    "    return text\n",
    "\n",
    "file_path = '/scratch/pvg2018/TLDR-Reddit/test.jsonl' \n",
    "prompt = ''\n",
    "i = 0\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        if i == 2:\n",
    "            prompt += formatting_prompts_func_without_TLDR(data) + '\\n'\n",
    "            break\n",
    "        else:\n",
    "            prompt += formatting_prompts_func(data) + '\\n'\n",
    "        i = i + 1\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c60269d4-13ef-4963-88f7-4d54e0b6d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.43s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> SUBREDDIT: r/relationships\\nTITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\\nPOST: Hello hello everybody. I hope this isn\\'t too trivial of a question to ask on here, but I\\'ve been feeling a bit out of my depth when it comes to this situation (I\\'ve had only one relationship before, and for many reasons, it was out of the ordinary).\\n\\nOkay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We\\'re both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\\n\\nOkay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \\n\\nWe ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\\n\\nYesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I\\'m wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I\\'ve been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I\\'m worried that I may have ruined my chances of a relationship by sleeping with him already.\\nTL;DR: I really like this guy, but after having sex with him after only knowing him for a very brief period of time, I am worried that I may have ruined my chances of a relationship with him.\\nSUBREDDIT: r/Parenting\\nTITLE: My 11 year old sons friend died suddenly, his funeral is today and my son suddenly doesn\\'t want to attend.\\nPOST: **repost from relationships**\\n\\nA couple of weeks ago my sons friend died in a freak accident, it was completely shocking and horrific. He isn\\'t aware of the details, but we broke the news to him as soon as we found out and have spoke about it many times with him.\\n\\nHe has cried about it, asked questions and spoken about it with his older siblings (who have also recently lost a friend) and seemed to be okay with it (considering the circumstances).\\n\\nLeading up to the funeral, we have talked about it and explained what he is to expect, etc. This is his first funeral, so we have made sure that he is aware of everything.\\n\\nBut today is the day, and he has broken down in the morning and says he doesn\\'t want to go. I have no idea what to do. Do I push him to go? I am worried that he will regret it later, as he is a kid who doesn\\'t like to do new things. But at the same time, I don\\'t want to pressure him to do something he doesn\\'t want to do.\\nAdvice, please?\\nTL;DR: Sons good friend died and his funeral is today. Son was always wanting to go until today. Do I push him to go, or let him skip it?\\nSUBREDDIT: r/relationships\\nTITLE: The girl [26 F] I [22 M] have been seeing for a month didn\\'t respond to me at all yesterday while hanging out with a friend [~30? M].\\nPOST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn\\'t call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue.\\n\\nI saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time.\\n\\nI don\\'t mind that she hangs out with friends, and I know it\\'s pretty early in the relationship, but am I wrong to be a little annoyed that she didn\\'t respond until 24 hours after my first text?\\nTL;DR:\\n* Girl I\\'m seeing for a month didn\\'t respond in four + hours to text.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/scratch/pvg2018/Mistral7b\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/scratch/pvg2018/Mistral7b\")        \n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\")#.to(\"cuda\")\n",
    "#model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a2d39d-6189-4a5f-8c37-ad9b5e1ee4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been saved to /scratch/pvg2018/TLDR-Reddit/SummaryPrompts.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    return f\"SUBREDDIT: r/{example['subreddit']}\\nTITLE: {example['title']}\\nPOST: {example['post']}\\nTL;DR: {example['summary']}\"\n",
    "\n",
    "def formatting_prompts_func_without_TLDR(example):\n",
    "    return f\"SUBREDDIT: r/{example['subreddit']}\\nTITLE: {example['title']}\\nPOST: {example['post']}\\nTL;DR:\"\n",
    "\n",
    "def process_file(file_path, output_file_path):\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Convert each line to a dictionary\n",
    "    data = [json.loads(line.strip()) for line in lines]\n",
    "    all_outputs = []\n",
    "\n",
    "    # Process each row in the data\n",
    "    for example in data:\n",
    "        # Randomly sample 2 different rows for processing\n",
    "        sampled_examples = random.sample([e for e in data if e != example], 2)\n",
    "        \n",
    "        # Apply formatting_prompts_func to the sampled rows\n",
    "        processed_sample = [formatting_prompts_func(sample) for sample in sampled_examples]\n",
    "        \n",
    "        # Append the formatting_prompts_func_without_TLDR of the current row\n",
    "        without_tldr_output = formatting_prompts_func_without_TLDR(example)\n",
    "        \n",
    "        # Collect the formatted texts\n",
    "        formatted_output = \"\\n\\n\".join(processed_sample) + \"\\n\\n\" + without_tldr_output\n",
    "        all_outputs.append(formatted_output)\n",
    "\n",
    "    # Save the processed data to a new JSONL file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for item in all_outputs:\n",
    "            outfile.write(json.dumps({\"processed_text\": item}) + '\\n')\n",
    "\n",
    "# Specify the file paths\n",
    "input_file_path = '/scratch/pvg2018/TLDR-Reddit/test.jsonl'\n",
    "output_file_path = '/scratch/pvg2018/TLDR-Reddit/SummaryPrompts.jsonl'\n",
    "\n",
    "# Process the file and save the output\n",
    "process_file(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"Processed data has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5776822-9d52-45d6-aa7b-4b4c842db25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:37<00:00, 18.68s/it]\n",
      "Generating responses:   0%|          | 0/6553 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating responses:   0%|          | 1/6553 [06:41<730:26:56, 401.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating responses:   0%|          | 2/6553 [13:29<737:05:59, 405.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_path = \"/scratch/pvg2018/Mistral7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def process_prompts(input_file_path, output_file_path):\n",
    "    # Read the input JSONL file with prompts\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for line in tqdm(lines, desc=\"Generating responses\"):\n",
    "        data = json.loads(line.strip())\n",
    "        prompt = data.get(\"processed_text\", \"\")  # Extract the prompt from each line\n",
    "        \n",
    "        # Tokenize the input and generate responses\n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        response = tokenizer.batch_decode(generated_ids)[0]\n",
    "        \n",
    "        # Save the prompt and response in the results\n",
    "        results.append({\"prompt\": prompt, \"response\": response})\n",
    "    \n",
    "    # Save results to a new JSON file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(results, outfile, indent=4)\n",
    "\n",
    "# Paths to your files\n",
    "input_file_path = '/scratch/pvg2018/TLDR-Reddit/SummaryPrompts.jsonl'\n",
    "output_file_path = '/scratch/pvg2018/TLDR-Reddit/SummaryMistral7b.jsonl'\n",
    "\n",
    "# Process the prompts and save the outputs\n",
    "process_prompts(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"Inferences have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8307ce5-c1d4-49ad-a457-cb143c92f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import ChatCompletion\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line.strip()) for line in file.readlines()]\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def create_gpt4_prompt(post, summary_a, summary_b):\n",
    "    prompt_text = f\"\"\"\n",
    "system\n",
    "You are a helpful assistant, that ranks models by the quality of their answers.\n",
    "\n",
    "user\n",
    "Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? A good summary is both precise and concise.\n",
    "Post: \"{post}\"\n",
    "Summary A:\n",
    "{{\n",
    "\"model\": \"LLM\",\n",
    "\"summary\": \"{summary_a}\"\n",
    "}}\n",
    "Summary B:\n",
    "{{\n",
    "\"model\": \"Original\",\n",
    "\"summary\": \"{summary_b}\"\n",
    "}}\n",
    "Now please rank the models by the quality of their summaries, so that the model with rank 1 has the best summary. Then return a list of the model names and ranks, i.e., produce the following output:\n",
    "[\n",
    "{{'model': <model-name>, 'rank': <model-rank>}},\n",
    "{{'model': <model-name>, 'rank': <model-rank>}}\n",
    "]\n",
    "Your response must be a valid Python dictionary and should contain nothing else because we will directly execute it in Python. Please provide the ranking that the majority of humans would give.\n",
    "\"\"\"\n",
    "    return prompt_text\n",
    "\n",
    "def evaluate_summaries(original_data, inference_data):\n",
    "    client = ChatCompletion()\n",
    "    results = []\n",
    "    for original, inferred in zip(original_data, inference_data):\n",
    "        prompt = create_gpt4_prompt(original['post'], inferred['response'], original['summary'])\n",
    "        response = client.create(prompt=prompt, model=\"gpt-4\", max_tokens=150)\n",
    "        result = eval(response.choices[0].text)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Paths to your files\n",
    "original_file_path = '/scratch/pvg2018/TLDR-Reddit/test.jsonl'\n",
    "inference_file_path = '/scratch/pvg2018/TLDR-Reddit/SummaryMistral7b.jsonl'\n",
    "\n",
    "# Read data\n",
    "original_data = read_jsonl(original_file_path)\n",
    "inference_data = read_json(inference_file_path)\n",
    "\n",
    "# Evaluate and compare summaries\n",
    "comparison_results = evaluate_summaries(original_data, inference_data)\n",
    "\n",
    "# Optionally, print or save the results\n",
    "print(comparison_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-example",
   "language": "python",
   "name": "pytorch-example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
